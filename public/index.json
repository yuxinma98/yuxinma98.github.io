[{"content":"","date":null,"permalink":"/publications/2025-transferability/","section":"Publications","summary":"","title":"On Transferring Transferability: Towards a Theory for Size Generalization"},{"content":"Hi! I am a second year PhD student at the Department of Applied Mathematics and Statistics at Johns Hopkins University, where I am fortunate to be advised by Soledad Villar and Tim Kunisky. My research interests lie at the intersection of mathematics and deep learning. I am exploring how to incorporate mathematical ideas into the design of new deep learning algorithms, while also investigating how deep learning can aid in solving challenging mathematical problems. Check out the about me page to learn more!\nPreviously, I completed an MMath and BA Hons degree in Mathematics at the University of Cambridge where I was mentored by Benedikt Löwe and Orsola Rath Spivack. During my master\u0026rsquo;s studies, I focused on Statistics and completed an essay on Topological Data Analysis, advised by John Aston and Jacob Rasmussen.\n","date":null,"permalink":"/","section":"profile","summary":"","title":"profile"},{"content":"","date":null,"permalink":"/publications/","section":"Publications","summary":"","title":"Publications"},{"content":"","date":null,"permalink":"/publications/2025-nonlinear-laplacian/","section":"Publications","summary":"","title":"Nonlinear Laplacians: Tunable principal component analysis under directional prior information"},{"content":"","date":null,"permalink":"/talks/","section":"Talks","summary":"","title":"Talks"},{"content":"Abstract: This talks serves as an introduction to Topological Data Analysis (TDA) – a method for analyzing the shape and structure of data using concepts from topology. TDA merges algebraic topology and other mathematical tools with real-world statistical analysis, offering both elegant mathematical theory and practical applications. The focus of this talk will be on Persistent Homology, widely regarded as the cornerstone technique in TDA. We\u0026rsquo;ll begin by exploring \u0026ldquo;why should we care about the topology or shape of data, and in what contexts is TDA valuable?\u0026rdquo; through examples such as brain artery tree data and prostate cancer histopathology images. Then, we\u0026rsquo;ll provide a gentle introduction to key concepts in homology theory (within algebraic topology) and demonstrate how these ideas evolve into the concept of persistent homology. Finally, we will examine how we could perform statistical analysis on persistent homology - through vectorization.\n","date":null,"permalink":"/talks/2024_tda/","section":"Talks","summary":"","title":"What is Topological Data Analysis?"},{"content":"I was born and raised in Nanjing, China. My journey has since taken me to Singapore and Cambridge, UK, where I had the privilege of living and studying before beginning my PhD in Baltimore, USA. During my undergraduate years studying mathematics, I developed a passion for two distinct directions: the rigorous study of abstract structures \u0026mdash; with abstract algebra, geometry, topology, and category theory among my favorites \u0026mdash; and the practical modeling of real-world data, involving statistics and machine learning. The convergence of these interests led me to explore interdisciplinary realms, as I came to believe that the deepest human understanding lies in the connections and shared insights across seemingly different domains. Drawn to the field of Geometric Deep Learning, I decided to pursue a PhD as a way to continually learn and explore elegant mathematical ideas while applying them to the complex, evolving challenges in modern deep learning.\nResesarch interests # I am deeply interested in exploring a wide range of mathematical concepts, understanding connections between diverse topics, and applying the rigorous study of abstract structures to data science and machine learning. Below, I outline some of the areas I am currently exploring and am passionate about. If you share an interest in these topics, I would be delighted to connect.\nAny-dimensional Learning: Developing neural network architectures that can be trained on inputs of small sizes and generalize effectively to larger sizes, ideally with theoretical guarantees on asymptotic performance.\nOn one hand, I am keen to understand the implicit inductive biases of existing neural networks in relation to their size generalization capabilities, to determine which tasks they are most suitable for. Check out our recent work in this direction [1]. On the other hand, I am interested in designing neural networks capable of solving specific any-dimensional problems, such as those encountered in combinatorial optimization and probabilistic/statistical modeling. I am particularly interested in the concept of \u0026ldquo;learning an algorithm\u0026rdquo; that works across input dimensions. Check our our recent work in this direction [2]. This area relates to topics such as graph neural networks (GNNs) for combinatorial optimization, the transferability and size generalization of GNNs, length generalization in large language models (LLMs), and neural algorithmic reasoning. Symmetries in Machine Learning: Investigating how to effectively leverage symmetries in various learning tasks and design equivariant neural networks—architectures that incorporate symmetry constraints—for mathematical applications.\nWithin this area, I aim to understand the expressive power, size generalization behavior, and other properties associated with different parameterization choices for equivariant networks. ","date":null,"permalink":"/about/","section":"About me","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]