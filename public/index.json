[{"content":"Hi! I am a PhD student at the Department of Applied Mathematics and Statistics at Johns Hopkins University, where I am fortunate to be advised by Soledad Villar and Tim Kunisky. My research interests lie at the intersection of mathematics and deep learning. I am exploring how to incorporate mathematical ideas into the design of new deep learning algorithms, while also investigating how deep learning can aid in solving challenging mathematical problems. Check out my research page to learn more!\nPreviously, I completed an MMath and BA Hons degree in Mathematics at the University of Cambridge where I was mentored by Benedikt Löwe and Orsola Rath Spivack. During my master\u0026rsquo;s studies, I focused on Statistics and completed an essay on Topological Data Analysis, advised by John Aston and Jacob Rasmussen.\n","date":null,"permalink":"/","section":"profile","summary":"","title":"profile"},{"content":"","date":null,"permalink":"/talks/","section":"Talks","summary":"","title":"Talks"},{"content":"Abstract: This talks serves as an introduction to Topological Data Analysis (TDA) – a method for analyzing the shape and structure of data using concepts from topology. TDA merges algebraic topology and other mathematical tools with real-world statistical analysis, offering both elegant mathematical theory and practical applications. The focus of this talk will be on Persistent Homology, widely regarded as the cornerstone technique in TDA. We\u0026rsquo;ll begin by exploring \u0026ldquo;why should we care about the topology or shape of data, and in what contexts is TDA valuable?\u0026rdquo; through examples such as brain artery tree data and prostate cancer histopathology images. Then, we\u0026rsquo;ll provide a gentle introduction to key concepts in homology theory (within algebraic topology) and demonstrate how these ideas evolve into the concept of persistent homology. Finally, we will examine how we could perform statistical analysis on persistent homology - through vectorization.\n","date":null,"permalink":"/talks/2024_tda/","section":"Talks","summary":"","title":"What is Topological Data Analysis?"},{"content":"My background #I was born and raised in Nanjing, China. My journey has taken me to Singapore and Cambridge, UK, where I had the privilege of living and studying before embarking on my PhD in Baltimore, USA.\nDuring my undergraduate studies, I spent four years doing the Mathematical Tripos, which introduced me to a diverse range of mathematical areas. I developed a passion for two distinct directions: the rigorous study of abstract structures (with abstract algebra, geometry, topology, and category theory as my favorites) and the practical modeling of real-world data (involving statistics and machine learning). The convergence of these interests inspired me to explore interdisciplinary realms, as I came to believe that the deepest human wisdom lies in the underlying connections and shared insights across different topics.\nResesarch interests #Here, I outline some of the topics I am currently exploring and feel passionately about. If you share an interest in these areas, we should chat!\nImposing symmetries in Neural Networks (equivariance in neural networks). Expressivity and universality of Graph Neural Networks and Equivariant Neural Networks. Algorithmic alignment with GNN. Applying Category Theory to build novel Neural Networks (Categorical Deep Learning) ","date":null,"permalink":"/about/","section":"About me","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" Research Interests #I am deeply interested in exploring a wide range of mathematical concepts, understanding connections between diverse topics, and applying the rigorous study of abstract structures (e.g., algebra, geometry) to data science and machine learning. Below, I outline some of the areas I am currently exploring and am passionate about. If you share an interest in these topics, I would be delighted to connect.\nAny-dimensional Learning: Developing neural network architectures that can be trained on inputs of small sizes and generalize effectively to larger sizes, ideally with theoretical guarantees on asymptotic performance.\nOn one hand, I am keen to understand the implicit inductive biases of existing neural networks in relation to their size generalization capabilities, to determine which tasks they are most suitable for. On the other hand, I am interested in designing neural networks capable of solving specific any-dimensional problems, such as those encountered in combinatorial optimization and probabilistic/statistical modeling. I am particularly interested in the concept of \u0026ldquo;learning an algorithm\u0026rdquo;: investigating whether a learned algorithm generalizes to input dimensions outside the training range, whether the learned algorithm can be interpreted to derive useful insights, and whether theoretical limits exist on the performance achievable by a learned algorithm. This area relates to topics such as graph neural networks (GNNs) for combinatorial optimization, the transferability and size generalization of GNNs, length generalization in large language models (LLMs), and neural algorithmic reasoning. Symmetries in Machine Learning: Investigating how to effectively leverage symmetries in various learning tasks and design equivariant neural networks—architectures that incorporate symmetry constraints—for mathematical applications.\nWithin this area, I aim to understand the expressive power, size generalization behavior, and other properties associated with different parameterization choices for equivariant networks. ","date":null,"permalink":"/research/","section":"Research","summary":"","title":"Research"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]