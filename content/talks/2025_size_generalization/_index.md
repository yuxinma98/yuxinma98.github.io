---
title: "Some thoughts on size generalization"
date: 2025-08-12
location: "Graph Learning Meets Theoretical Computer Science Workshop, Simons Institute"
slidesUrl: "https://livejohnshopkins-my.sharepoint.com/:b:/g/personal/yma93_jh_edu/EcgSu_L5S-JGp94tD_eFee4BbmMHpOjjVc-US6vH2p3Axw"
draft: false
---
**Abstract**: This talks explore the question: For existing models capable of handling inputs of arbitrary sizes (e.g. DeepSet, GNN, transformer), what properties of the data and task enable eff ective size generalization? My answer involves two key points: (1) task-model alignment; (2) training and test distributions are “similar” relative to the model’s inductive bias.

[Slides](https://livejohnshopkins-my.sharepoint.com/:b:/g/personal/yma93_jh_edu/EcgSu_L5S-JGp94tD_eFee4BbmMHpOjjVc-US6vH2p3Axw)