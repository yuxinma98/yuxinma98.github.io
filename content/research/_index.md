---
title: "Research"
---
<!-- ## My background

I was born and raised in Nanjing, China. My journey has taken me to Singapore and Cambridge, UK, where I had the privilege of living and studying before embarking on my PhD in Baltimore, USA.

During my undergraduate studies, I spent four years doing the [Mathematical Tripos](https://www.maths.cam.ac.uk/undergrad/undergrad), which introduced me to a diverse range of mathematical areas. I developed a passion for two distinct directions: the rigorous study of **abstract structures** (with abstract algebra, geometry, topology, and category theory as my favorites) and the practical modeling of **real-world data** (involving statistics and machine learning). The convergence of these interests inspired me to explore interdisciplinary realms, as I came to believe that the deepest human wisdom lies in the underlying connections and shared insights across different topics. -->

<!-- In my fourth year at Cambridge, I was introduced to the field of **Geometric Deep Learning** through [Prof. Michael Bronstein's illuminating talk](https://www.youtube.com/watch?v=w6Pw4MOzMuo). This area, precisely at the intersection of my interests, captivated me with its expansive mathematical theories and exciting applications propelling advancements in artificial intelligence. Intrigued by the possibilities it presented, I decided to pursue a PhD to delve deeper into this area. I was very fortunate to have joined Soledad's group at JHU, and I am now relishing every moment of my time here! -->
## Research Interests

I am deeply interested in exploring a wide range of mathematical concepts, understanding connections between diverse topics, and applying the rigorous study of abstract structures (e.g., algebra, geometry) to data science and machine learning. Below, I outline some of the areas I am currently exploring and am passionate about. If you share an interest in these topics, I would be delighted to connect.

* **Any-dimensional Learning**: Developing neural network architectures that can be trained on inputs of small sizes and generalize effectively to larger sizes, ideally with theoretical guarantees on asymptotic performance. 
    * On one hand, I am keen to understand the implicit inductive biases of existing neural networks in relation to their size generalization capabilities, to determine which tasks they are most suitable for. On the other hand, I am interested in designing neural networks capable of solving specific any-dimensional problems, such as those encountered in combinatorial optimization and probabilistic/statistical modeling.
    * I am particularly interested in the concept of "learning an algorithm": investigating whether a learned algorithm generalizes to input dimensions outside the training range, whether the learned algorithm can be interpreted to derive useful insights, and whether theoretical limits exist on the performance achievable by a learned algorithm.
    * This area relates to topics such as graph neural networks (GNNs) for combinatorial optimization, the transferability and size generalization of GNNs, length generalization in large language models (LLMs), and neural algorithmic reasoning.

* **Symmetries in Machine Learning**: Investigating how to effectively leverage symmetries in various learning tasks and design equivariant neural networks—architectures that incorporate symmetry constraints—for mathematical applications.
    * Within this area, I aim to understand the expressive power, size generalization behavior, and other properties associated with different parameterization choices for equivariant networks.