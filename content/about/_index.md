---
title: "About me"
---
I was born and raised in Nanjing, China. My journey has since taken me to Singapore and Cambridge, UK, where I had the privilege of living and studying before beginning my PhD in Baltimore, USA. During my undergraduate years studying mathematics, I developed a passion for two distinct directions: the rigorous study of **abstract structures** --- with abstract algebra, geometry, topology, and category theory among my favorites --- and the practical modeling of **real-world data**, involving statistics and machine learning. The convergence of these interests led me to explore interdisciplinary realms, as I came to believe that the deepest human understanding lies in the connections and shared insights across seemingly different domains. Drawn to the field of **Geometric Deep Learning**, I decided to pursue a PhD as a way to continually learn and explore elegant mathematical ideas while applying them to the complex, evolving challenges in modern deep learning.
<!-- In my fourth year at Cambridge, I was introduced to the field of **Geometric Deep Learning** through [Prof. Michael Bronstein's illuminating talk](https://www.youtube.com/watch?v=w6Pw4MOzMuo). This area, precisely at the intersection of my interests, captivated me with its expansive mathematical theories and exciting applications propelling advancements in artificial intelligence. Intrigued by the possibilities it presented, I decided to pursue a PhD to delve deeper into this area. I was very fortunate to have joined Soledad's group at JHU, and I am now relishing every moment of my time here! -->

## Resesarch interests
<!-- Here, I outline some of the topics I am currently exploring and feel passionately about. If you share an interest in these areas, we should chat!

* Imposing symmetries in Neural Networks (equivariance in neural networks).
* Expressivity and universality of Graph Neural Networks and Equivariant Neural Networks.
* Algorithmic alignment with GNN.
* Applying Category Theory to build novel Neural Networks (Categorical Deep Learning) -->

I am deeply interested in exploring a wide range of mathematical concepts, understanding connections between diverse topics, and applying the rigorous study of abstract structures to data science and machine learning. Below, I outline some of the areas I am currently exploring and am passionate about. If you share an interest in these topics, I would be delighted to connect.

* **Any-dimensional Learning**: Developing neural network architectures that can be trained on inputs of small sizes and generalize effectively to larger sizes, ideally with theoretical guarantees on asymptotic performance. 
    * On one hand, I am keen to understand the implicit inductive biases of existing neural networks in relation to their size generalization capabilities, to determine which tasks they are most suitable for. Check out our recent work in this direction [[1]](https://arxiv.org/abs/2505.23599). 
    * On the other hand, I am interested in designing neural networks capable of solving specific any-dimensional problems, such as those encountered in combinatorial optimization and probabilistic/statistical modeling. I am particularly interested in the concept of "learning an algorithm" that works across input dimensions. Check our our recent work in this direction [[2]](https://arxiv.org/abs/2505.12528).
    * This area relates to topics such as graph neural networks (GNNs) for combinatorial optimization, the transferability and size generalization of GNNs, length generalization in large language models (LLMs), and neural algorithmic reasoning.

* **Symmetries in Machine Learning**: Investigating how to effectively leverage symmetries in various learning tasks and design equivariant neural networks—architectures that incorporate symmetry constraints—for mathematical applications.
    * Within this area, I aim to understand the expressive power, size generalization behavior, and other properties associated with different parameterization choices for equivariant networks.